{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Quantitative Investing with Python</span>\n",
    "\n",
    "### Professor Juhani Linnainmaa\n",
    "\n",
    "Dartmouth College and Kepos Capital (Co-Director of Research)\n",
    "\n",
    "--- \n",
    "\n",
    "# **Topic 3:** Reversals and Momentum\n",
    "\n",
    "The goal in this section of the course is to\n",
    "1. Get familiarity with constructing trading strategies...\n",
    "2. By replicating some academic factors\n",
    "\n",
    "In this part I will use monthly CRSP data -- CRSP stands for Center for Research in Security Prices.\n",
    "- Most universities and colleges subscribe to data such as CRSP through Wharton's WRDS service\n",
    "- I'm providing the full monthly CRSP file from May 1962 through September 2023 \n",
    "  - However, I only include a fraction of the fields available\n",
    "  \n",
    "I will consider \"price-based\" factors, that is, trading rules that are based only past security price information\n",
    "\n",
    "The major price-based factors include\n",
    "- Size\n",
    "- Short- and long-term reversals\n",
    "- Momentum\n",
    "- Idiosyncratic volatility\n",
    "- Betting against beta\n",
    "\n",
    "When you construct a factor -- as discussed in Lecture 1 -- you need to make *many* choices \n",
    "- Moreover, the data may change over time and so, in practice, it is *very* difficult to replicate a factor perfectly unless you have the original data and code\n",
    "- Sometimes the original papers (and industry reports) do not provide enough details for replicating the factors\n",
    "  - For example, Li, Novy-Marx, and Velikov (2019) (https://cfr.pub/published/papers/li2020liquidity.pdf) struggled to replicate a famous factor paper until they figured out that the authors had used an unreported rule:\n",
    "  \n",
    "  \n",
    "  ```\n",
    "  \"Finally, while not noted by PS, they delete zero-volume observations when estimating Eq. (1), and doing so here is crucial to generating a high correspondence between our results and those reported in their paper.\n",
    "  \n",
    "  Determining this fact required implementing numerous variations on the methodology described in PS. This involved labor far beyond what could reasonably be expected for casual replication, and was only possible because of the public aggregate liquidity series maintained by PS, which allowed us to infer which variations were important for generating a close correspondence.\" (p. 227)\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from io import BytesIO, StringIO\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process monthly stock data from CRSP\n",
    "\n",
    "- I downloaded it from WRDS, zipped it, and put into Dropbox\n",
    "- The code below downloads the file and unzips it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_url = 'https://dl.dropboxusercontent.com/scl/fi/jnzk25egqsup3j4ibyvni/CRSP_September2023.csv.zip?rlkey=9im4ectsyl9ls7o9aw7x67odg'\n",
    "response = requests.get(crsp_url)\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    with z.open('CRSP_September2023.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- I don't really need company names, but I'll put them into a Series just in case I want to look something up based on PERMNO\n",
    "- I also don't need TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = df[['PERMNO','COMNAM']]\n",
    "company_names = company_names.groupby('PERMNO').last().squeeze()\n",
    "df = df.drop(columns=['COMNAM','TICKER'])\n",
    "company_names.name = 'Company names'\n",
    "company_names.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing 2\n",
    "\n",
    "1. change dates to datetime\n",
    "2. set permno-date as the index (it becomes a multi-index)\n",
    "3. the typical universe in equities is to keep common stock traded on NYSE, Nasdaq, and AMEX -- filter based on SHRCD and EXCHCD\n",
    "4. drop SHRCD and EXCHCD because we don't need them anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index(['PERMNO', 'date'])\n",
    "\n",
    "# we want to keep SHRCD = 10 or 11 and EXCHCD=1,2, or 3 -- the raw input data should already have these filters\n",
    "df = df[(df['SHRCD'].isin([10,11])) & (df['EXCHCD'].isin([1,2,3]))]\n",
    "\n",
    "# drop the SHRCD variable - we don't need it anymore\n",
    "df = df.drop(columns=['SHRCD'])\n",
    "\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert two returns variable, DLRET and RET, into floats\n",
    "\n",
    "- There are some strings, which we ignore (this is the 'coerce' argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what *is* DLRET?\n",
    "print(df['DLRET'].describe())\n",
    "\n",
    "col_list = ['DLRET', 'RET']\n",
    "for col in col_list:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute returns inclusive of delisting returns\n",
    "\n",
    "- Our return variable is either 'normal return', 'delisting return', or, if both exist, the compounded return\n",
    "- We need to be careful with missign values - I'm filling in zeros for NaNs in the computation but then, if neither return exists, putting them back in  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret'] = (1 + df['RET'].fillna(0)) * (1 + df['DLRET'].fillna(0)) - 1\n",
    "\n",
    "neither_return_exists = (df['RET'].isnull()) & (df['DLRET'].isnull())\n",
    "\n",
    "df.loc[neither_return_exists, 'ret'] = np.nan\n",
    "\n",
    "# drop the original return variables - we don't need them anymore\n",
    "df = df.drop(columns=['RET', 'DLRET'])\n",
    "\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute market cap in millions\n",
    "- Note that PRC is negative to indicate that it is the spread midpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['me'] = np.abs(df['PRC']) * df['SHROUT'] / 1_000\n",
    "\n",
    "# Shares outstanding is sometimes zero or PRC missing -> set me to missing\n",
    "df['me'] = df['me'].replace({0: np.nan})\n",
    "\n",
    "# Drop PRC -> we don't need it anymore\n",
    "df = df.drop(columns=['PRC', 'SHROUT'])\n",
    "\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What stock has a market cap of \\\\$794,196.8M at the end of September 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names.loc[93436]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save finished file into a PKL file\n",
    "\n",
    "- Be careful with pickle files - they are very convenient but not efficient and break between Python and Pandas versions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/crsp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/crsp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading strategies\n",
    "\n",
    "A trading strategy is a systematic rule based on data we know at the time we make the trading decisions\n",
    "\n",
    "Your strategy could be \"value investing:\"\n",
    "\n",
    "1. Buy value stocks, sell growth stocks (this is known as a long-short portfolio)\n",
    "2. Look at the data once a month to rebalance your portfolios as stocks' characteristics change\n",
    "\n",
    "### Strategy 1: Short-term reversals\n",
    "\n",
    "**Short-term reversals** is the empirical finding that, at short horizons, stock returns tend **reverse**\n",
    "\n",
    "In monthly data, a strategy that trades short-term reversals is simple:\n",
    "\n",
    "- Rank stocks by their prior-month returns (e.g., their returns in December)\n",
    "- At the end of the month, buy stocks with the lowest returns and sell stocks with the highest returns\n",
    "\n",
    "To get started, I'll create the following strategy:\n",
    "\n",
    "1. Every month, based on the entire universe, identify the bottom and top 10% of stocks based on their returns\n",
    "2. Create two portfolios:\n",
    "   - an equal-weighted 'long' portfolio that buys the bottom stocks\n",
    "   - an equal-weighted 'short' portfolio that sells the top stocks\n",
    "3. The return on the strategy will be the return on these stocks the *following* month\n",
    "\n",
    "We need to pay some attention to timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine top and bottom deciles\n",
    "\n",
    "- I group stocks by month and compute the 10th and 90th percentiles\n",
    "- I get a new dataframe that shows how low or high a stock's return must be to be in the tails of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = df['ret'].dropna().groupby(level=1)\n",
    "p10 = grp.apply(lambda x: np.percentile(x, 10))\n",
    "p90 = grp.apply(lambda x: np.percentile(x, 90))\n",
    "breakpoints = pd.DataFrame({'p10': p10, 'p90': p90})\n",
    "breakpoints.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge breakpoints back into our original dataframe\n",
    "- We need to specify what we are merging on.\n",
    "- On the \"left\" we are merging by (level) \"date\"; on the \"right\" we are merging by the index (which is also date)\n",
    "- We also need to specify what observations we want to keep: those on the left, those on the right, the union (inner), or the join (outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(breakpoints, left_on='date', right_index=True, how='left')\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 0/1 variables to indicate stocks that we want to hold LONG or SHORT *next month*\n",
    "\n",
    "- I also create a -1 / +1 variable called 'position' for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['long'] = (df['ret'] <= df['p10']).astype('int')\n",
    "df['short'] = (df['ret'] >= df['p90']).astype('int')\n",
    "df['position'] = df['long'] - df['short']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create equal-weight portfolios\n",
    "\n",
    "- Each stock in a portfolio gets a weight of 1/N\n",
    "- So I need to compute how many stocks we have in the two portfolios each month\n",
    "- I can groupby 'date' (month) and take sums of my long and short variables\n",
    "- I can then get weights by dividing the original dataframe with these counts\n",
    "  - The columns align: we divide long by long and short by short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many stocks do we have in each portfolio each month?\n",
    "ns = df[['long','short']].groupby(level='date').sum()\n",
    "print(ns.tail(5))\n",
    "\n",
    "weights = df[['long','short']] / ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the weights sum up to 1\n",
    "weights.groupby(level=1).sum().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge weights back into the main dataframe\n",
    "\n",
    "- I drop, on the fly, some columns that we don't need and that have conflicting names (namely, \"long\" and \"short\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['p10', 'p90', 'long','short']).merge(weights,left_index=True,right_index=True)\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute returns for the two portfolios\n",
    "\n",
    "- Here timing is important: we have weights as of month t, that is, the return we have on each row is the same return we looked at to determine the weight\n",
    "- We want to either shift 'ret' forward one month or the weights back one month\n",
    "- I shift weights back one month\n",
    "  - **Important**: I need to shift within each PERMNO so that when PERMNO changes, I don't grab the weights from the previous row\n",
    "  - I could alternative reshape the dataframe so that I only had dates in the index\n",
    "    - But this would be slower and take more memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = df[['long', 'short']].groupby(level='PERMNO').shift(1)\n",
    "w.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return computation\n",
    "\n",
    "- Portfolio return is the sum of weights * returns each month \n",
    "- I multiple (now lagged) weights with returns and take the sum of these products\n",
    "- min_count = 1 is used to make sure that if there are any months without anything to sum, the result is a missing value (NaN)\n",
    "  - By default, a sum of missing values equals zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns = w.mul(df['ret'], axis=0).groupby(level='date').sum(min_count=1)\n",
    "strev = portfolio_returns['long'] - portfolio_returns['short']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the strategy's performance\n",
    "\n",
    "I define a helper function for doing some analysis\n",
    "- It just means that I don't have to rewrite the same code\n",
    "- It is good to write modular code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_returns(r=None, name=None, start_date='1962-05', end_date='2023-09'):\n",
    "    r = r.loc[start_date:end_date]\n",
    "    ir = np.sqrt(12) * r.mean() / r.std()\n",
    "    print(f'Analysis of a strategy: \"{name}\"')\n",
    "    print(f'Start: {start_date}, End: {end_date}')\n",
    "    print(f'Sharpe ratio: {ir:.2f}')\n",
    "    r.cumsum().plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_returns(strev, 'Short-term reversals (deciles, equal-weighted)', end_date='1995-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What issues might this strategy have?\n",
    "\n",
    "- A *huge* amount of turnover\n",
    "- Trading all stocks the same way independent of market caps\n",
    "- Additional practical issue: we cannot implement this specific rule in real life\n",
    "  - There is no gap between on observing the signal and when we assume we got into the positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a *value-weighted* strategy instead\n",
    "\n",
    "- The amount we invest in each stock is proportional to its market capitalization\n",
    "- This how almost all academic factors are construct and it is a far fairer representation of how well the strategy might perform\n",
    "  - But it is still \"gross\" of trading costs\n",
    "- I'll construct this portfolio slightly differently\n",
    "  1. I create a new column that contains next month's return for each stock (shift = -1 now)\n",
    "  2. I take the product of market caps and these future returns\n",
    "  3. I take the sum of these products (and market caps) seperately for the stocks that belong to the long and short portfolios\n",
    "  4. The value-weighted return is sum(me * retnm) / sum(me)\n",
    "  \n",
    "I end up with two series: long_return and short_return\n",
    "\n",
    "- I *could* have again computed weights based on market caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add next month's return\n",
    "df['retnm'] = df['ret'].groupby(level='PERMNO').shift(-1)\n",
    "df['me_x_retnm'] = df['me'] * df['retnm']\n",
    "\n",
    "# long portfolio\n",
    "long = df['position'] == 1\n",
    "long_sums = df.dropna().loc[long,['me','me_x_retnm']].groupby(level='date').sum()\n",
    "long_return = long_sums['me_x_retnm'] / long_sums['me']\n",
    "\n",
    "# short portfolio\n",
    "short = df['position'] == -1\n",
    "short_sums = df.dropna().loc[short,['me','me_x_retnm']].groupby(level='date').sum()\n",
    "short_return = short_sums['me_x_retnm'] / short_sums['me']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the value-weighted strategy\n",
    "\n",
    "- I need to SHIFT strategy returns so that the return each month corresponds to what the index says\n",
    "- This undoes my 'retnm' convention from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strev_vw = long_return - short_return\n",
    "strev_vw = strev_vw.shift(1)\n",
    "analyze_returns(strev_vw, 'Short-term reversals (deciles, value-weighted)', end_date='1995-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How highly are the equal- and value-weighted strategies correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([strev, strev_vw], axis=1).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Fama and French's computation\n",
    "\n",
    "- Did we do this right? \n",
    "\n",
    "### Get Fama-French factors from Ken French's website \n",
    "\n",
    "- I write a helper function that I can use to download the data\n",
    "- There are some file-specific issues that I need to control for \n",
    "- I get both Fama-French factors (for later use) and portfolios formed based on short-term reversals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_french_data(url=None, csvname=None, skiplines=None):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the request is NOT successful, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to download zip file. Status code: {response.status_code}\")\n",
    "\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "\n",
    "        # Check if the file exists in the zip archive\n",
    "        if csvname in zip_file.namelist():\n",
    "            # Read the CSV file directly from the zip archive\n",
    "            with zip_file.open(csvname) as csv_file:\n",
    "                lines = csv_file.readlines()\n",
    "\n",
    "            # Remove rows from the beginning\n",
    "            lines = lines[skiplines:]\n",
    "\n",
    "            # Create a DataFrame from the trimmed lines using StringIO\n",
    "            # First need to decode byte strings into unicode\n",
    "            lines = [line.decode(\"utf-8\") for line in lines]\n",
    "\n",
    "            # at some point the file switches from monthly factors to annual factors and other stuff\n",
    "            # we can delete what ever comes after\n",
    "            for idx, line in enumerate(lines):\n",
    "                if ('Annual Factors' in line) or (len(line.strip())==0): break\n",
    "                \n",
    "            lines = lines[:idx]\n",
    "            clean_csv = '\\n'.join(lines)\n",
    "            df = pd.read_csv(StringIO(clean_csv))                \n",
    "            print(f'File {csvname} read successfully!')\n",
    "            return df\n",
    "        else:\n",
    "            print(f'Zip file found but file {csvname} not found in the archive.')   \n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read FF5 factors and clean the data\n",
    "\n",
    "- Convert returns to decimals and date from YYYYMM to datatime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file we want to read -- the CSV file inside has almost the same name \n",
    "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip'\n",
    "csvname = 'F-F_Research_Data_5_Factors_2x3.csv'\n",
    "\n",
    "df_ff5 = download_french_data(url=url, csvname=csvname, skiplines=3)\n",
    "\n",
    "print('\\nData before processing:\\n')\n",
    "print(df_ff5.head(3))\n",
    "\n",
    "df_ff5['date'] = df_ff5['Unnamed: 0'].apply(lambda x: datetime.strptime(str(x), '%Y%m'))\n",
    "ff_data = df_ff5.loc[:,'Mkt-RF':'date'].set_index('date') / 100\n",
    "\n",
    "print('\\nData after processing:\\n')\n",
    "print(ff_data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read returns on portfolios formed based on short-term reversals\n",
    "\n",
    "- Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/10_Portfolios_Prior_1_0_CSV.zip'\n",
    "csvname = '10_Portfolios_Prior_1_0.CSV'    \n",
    "\n",
    "df_rev = download_french_data(url=url, csvname=csvname, skiplines=10)\n",
    "\n",
    "print('Data before processing:\\n')\n",
    "print(df_rev.head(3))\n",
    "\n",
    "df_rev['date'] = df_rev['Unnamed: 0'].apply(lambda x: datetime.strptime(str(x), '%Y%m'))\n",
    "df_rev = df_rev.drop(columns='Unnamed: 0')\n",
    "rev_data = df_rev.set_index('date') / 100\n",
    "\n",
    "print('\\nData after processing:\\n')\n",
    "print(rev_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strev_ff = rev_data['Lo PRIOR'] - rev_data['Hi PRIOR']\n",
    "analyze_returns(strev_ff, 'Short-term reversals (deciles, value-weighted, FF)', end_date='1995-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between our strategy and that of Fama and French\n",
    "\n",
    "- There is the small issue that the dates are different\n",
    "- I change them to monthly so I can merge the two series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = strev_vw.copy()\n",
    "ours.index = ours.index.to_period('M').to_timestamp('M')\n",
    "ours.name = 'Our strategy'\n",
    "\n",
    "theirs = strev_ff.copy()\n",
    "theirs.index = theirs.index.to_period('M').to_timestamp('M')\n",
    "theirs.name = 'FF\\'s strategy'\n",
    "pd.concat([ours, theirs], axis=1).corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is the difference?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
